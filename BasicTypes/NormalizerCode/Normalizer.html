<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <title></title>
</head>
<body>
    <h2>The normalizer pipe line</h2>
    <p>
        There are three big steps.
    </p>
    <ol>
        <li>First, split the text into sentences and infer paragraphs, etc depending on the discourse type (prose, poetry, etc)</li>
        <li>Second, if the text is not annotated, infer prepositions, etc. This is not guaranteed to be idempotent, i.e. double normalizing may yield garbage.</li>
        <li>Third, do the normalizations that assume the text has been annotated. It should be idempotent, ie. double normalizing yields the same result.</li>
    </ol>


    <p>The normalizer processes arbitrary text and creates punctuated toki pona.</p>
    <p>
        The normalizer has a chicken & egg problem. It can't normalize accurately without knowing part of speech, where sentences end, etc.
        But that requires parsing. But to parse, we need either general purpose artificial intelligence, or predictable, relatively error free
        toki pona.
    </p>
    <p>
        Normalize Foreign is for normalizing diglossia and would be able to assume that there is just a little toki pona. This one will incorrectly assume toki pona errors are foreign text.
        The regular normalizer assumes that the text is mostly toki pona. This one will incorrectly assume that foreign text is toki pona errors.
    </p>
    <ol>
        <li>Adds li to mi/sina</li>
        <li>Ad hoc undo of overnormalized mi li/sina li</li>
        <li>Punctuates prepositions</li>
        <li>Ad hoc undo of overnormalized prepositions (tagged as preposition, but it isn't)</li>
        <li>Removes extraneous whitespace, line breaks</li>
        <li>Removes most extraneous commas</li>
    </ol>
</body>
</html>